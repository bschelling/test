{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d930beae",
   "metadata": {},
   "source": [
    "# Jewelry Image Similarity with Siamese Networks\n",
    "\n",
    "This notebook demonstrates how to build an image similarity model for finding visually similar jewelry items.\n",
    "\n",
    "## Overview\n",
    "- **Task**: Image similarity / retrieval\n",
    "- **Approach**: Siamese network with contrastive loss\n",
    "- **Use Case**: Find similar jewelry items based on visual appearance\n",
    "- **Output**: Vector embeddings that can be compared using distance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354baf80",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faefc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff1e330",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216cef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data paths\n",
    "DATA_DIR = Path(\"/project/data/raw_jewelry\")\n",
    "MODEL_DIR = Path(\"/project/models\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Explore the dataset structure\n",
    "categories = sorted([d.name for d in DATA_DIR.iterdir() if d.is_dir()])\n",
    "print(f\"Number of categories: {len(categories)}\")\n",
    "print(f\"Categories: {categories}\\n\")\n",
    "\n",
    "# Collect all images with their categories\n",
    "all_images = []\n",
    "for category in categories:\n",
    "    category_path = DATA_DIR / category\n",
    "    image_files = list(category_path.glob(\"*.jpg\")) + list(category_path.glob(\"*.png\"))\n",
    "    for img_path in image_files:\n",
    "        all_images.append({'path': img_path, 'category': category})\n",
    "    print(f\"{category}: {len(image_files)} images\")\n",
    "\n",
    "print(f\"\\nTotal images: {len(all_images)}\")\n",
    "\n",
    "# Create lookup by category for triplet sampling\n",
    "images_by_category = defaultdict(list)\n",
    "for img_info in all_images:\n",
    "    images_by_category[img_info['category']].append(img_info['path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02645721",
   "metadata": {},
   "source": [
    "## 3. Configuration and Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2de613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16  # Smaller batch for triplet learning\n",
    "EPOCHS = 30\n",
    "EMBEDDING_DIM = 128  # Size of embedding vector\n",
    "LEARNING_RATE = 0.0001\n",
    "MARGIN = 1.0  # Margin for triplet loss\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Data transforms (same for all images)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Embedding dimension: {EMBEDDING_DIM}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Triplet margin: {MARGIN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d072f1d4",
   "metadata": {},
   "source": [
    "## 4. Triplet Dataset\n",
    "\n",
    "For similarity learning, we use triplets: (anchor, positive, negative)\n",
    "- **Anchor**: Reference image\n",
    "- **Positive**: Different image of same category (similar)\n",
    "- **Negative**: Image from different category (dissimilar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bda600",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, images_by_category, categories, transform=None):\n",
    "        self.images_by_category = images_by_category\n",
    "        self.categories = categories\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Flatten to get all image paths\n",
    "        self.all_images = []\n",
    "        for cat in categories:\n",
    "            self.all_images.extend([(path, cat) for path in images_by_category[cat]])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.all_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get anchor\n",
    "        anchor_path, anchor_category = self.all_images[idx]\n",
    "        \n",
    "        # Get positive (same category, different image)\n",
    "        positive_candidates = [p for p in self.images_by_category[anchor_category] if p != anchor_path]\n",
    "        if len(positive_candidates) == 0:\n",
    "            positive_path = anchor_path  # Fallback if only one image in category\n",
    "        else:\n",
    "            positive_path = random.choice(positive_candidates)\n",
    "        \n",
    "        # Get negative (different category)\n",
    "        negative_category = random.choice([c for c in self.categories if c != anchor_category])\n",
    "        negative_path = random.choice(self.images_by_category[negative_category])\n",
    "        \n",
    "        # Load and transform images\n",
    "        anchor = Image.open(anchor_path).convert('RGB')\n",
    "        positive = Image.open(positive_path).convert('RGB')\n",
    "        negative = Image.open(negative_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            anchor = self.transform(anchor)\n",
    "            positive = self.transform(positive)\n",
    "            negative = self.transform(negative)\n",
    "        \n",
    "        return anchor, positive, negative\n",
    "\n",
    "# Split data\n",
    "train_categories = categories  # Using all categories for training\n",
    "train_dataset = TripletDataset(images_by_category, train_categories, transform=transform)\n",
    "val_dataset = TripletDataset(images_by_category, train_categories, transform=val_transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401e004b",
   "metadata": {},
   "source": [
    "## 5. Siamese Network Architecture\n",
    "\n",
    "Uses a shared CNN backbone to create embeddings for all three images in a triplet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c467e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim=128):\n",
    "        super(EmbeddingNetwork, self).__init__()\n",
    "        \n",
    "        # Use pre-trained MobileNetV2 as backbone\n",
    "        mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "        \n",
    "        # Remove the classifier\n",
    "        self.features = mobilenet.features\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Embedding head\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(1280, 512),  # MobileNetV2 outputs 1280 features\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, embedding_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.embedding(x)\n",
    "        # L2 normalize embeddings\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = EmbeddingNetwork(embedding_dim=EMBEDDING_DIM)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "print(\"Model created successfully!\")\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f5704",
   "metadata": {},
   "source": [
    "## 6. Triplet Loss\n",
    "\n",
    "Loss function that encourages:\n",
    "- Anchor-Positive distance to be small\n",
    "- Anchor-Negative distance to be large\n",
    "- Margin between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bf1641",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        # Euclidean distance\n",
    "        pos_dist = F.pairwise_distance(anchor, positive, p=2)\n",
    "        neg_dist = F.pairwise_distance(anchor, negative, p=2)\n",
    "        \n",
    "        # Triplet loss: max(d(a,p) - d(a,n) + margin, 0)\n",
    "        losses = F.relu(pos_dist - neg_dist + self.margin)\n",
    "        \n",
    "        return losses.mean(), pos_dist.mean(), neg_dist.mean()\n",
    "\n",
    "criterion = TripletLoss(margin=MARGIN)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "print(f\"Triplet loss with margin: {MARGIN}\")\n",
    "print(f\"Optimizer: Adam (lr={LEARNING_RATE})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436f7438",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7895a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_pos_dist = 0.0\n",
    "    running_neg_dist = 0.0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for anchor, positive, negative in pbar:\n",
    "        anchor = anchor.to(device)\n",
    "        positive = positive.to(device)\n",
    "        negative = negative.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get embeddings\n",
    "        anchor_emb = model(anchor)\n",
    "        positive_emb = model(positive)\n",
    "        negative_emb = model(negative)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss, pos_dist, neg_dist = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_pos_dist += pos_dist.item()\n",
    "        running_neg_dist += neg_dist.item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': running_loss / (pbar.n + 1),\n",
    "            'pos_dist': running_pos_dist / (pbar.n + 1),\n",
    "            'neg_dist': running_neg_dist / (pbar.n + 1)\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_pos_dist = running_pos_dist / len(loader)\n",
    "    epoch_neg_dist = running_neg_dist / len(loader)\n",
    "    return epoch_loss, epoch_pos_dist, epoch_neg_dist\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_pos_dist = 0.0\n",
    "    running_neg_dist = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc='Validation')\n",
    "        for anchor, positive, negative in pbar:\n",
    "            anchor = anchor.to(device)\n",
    "            positive = positive.to(device)\n",
    "            negative = negative.to(device)\n",
    "            \n",
    "            anchor_emb = model(anchor)\n",
    "            positive_emb = model(positive)\n",
    "            negative_emb = model(negative)\n",
    "            \n",
    "            loss, pos_dist, neg_dist = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_pos_dist += pos_dist.item()\n",
    "            running_neg_dist += neg_dist.item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': running_loss / (pbar.n + 1),\n",
    "                'pos_dist': running_pos_dist / (pbar.n + 1),\n",
    "                'neg_dist': running_neg_dist / (pbar.n + 1)\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_pos_dist = running_pos_dist / len(loader)\n",
    "    epoch_neg_dist = running_neg_dist / len(loader)\n",
    "    return epoch_loss, epoch_pos_dist, epoch_neg_dist\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\\n\")\n",
    "history = {\n",
    "    'train_loss': [], 'train_pos_dist': [], 'train_neg_dist': [],\n",
    "    'val_loss': [], 'val_pos_dist': [], 'val_neg_dist': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    train_loss, train_pos, train_neg = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    val_loss, val_pos, val_neg = validate_epoch(model, val_loader, criterion, DEVICE)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_pos_dist'].append(train_pos)\n",
    "    history['train_neg_dist'].append(train_neg)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_pos_dist'].append(val_pos)\n",
    "    history['val_neg_dist'].append(val_neg)\n",
    "    \n",
    "    print(f\"Train - Loss: {train_loss:.4f}, Pos Dist: {train_pos:.4f}, Neg Dist: {train_neg:.4f}\")\n",
    "    print(f\"Val   - Loss: {val_loss:.4f}, Pos Dist: {val_pos:.4f}, Neg Dist: {val_neg:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), MODEL_DIR / 'best_similarity_model.pth')\n",
    "        print(f\"✓ Saved best model with validation loss: {val_loss:.4f}\")\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    print()\n",
    "\n",
    "print(\"\\n✓ Training completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bb6a06",
   "metadata": {},
   "source": [
    "## 8. Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacce0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(epochs_range, history['train_loss'], label='Training Loss', linewidth=2, marker='o')\n",
    "axes[0].plot(epochs_range, history['val_loss'], label='Validation Loss', linewidth=2, marker='s')\n",
    "axes[0].set_title('Triplet Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distances\n",
    "axes[1].plot(epochs_range, history['train_pos_dist'], label='Train Positive Distance', linewidth=2, marker='o')\n",
    "axes[1].plot(epochs_range, history['train_neg_dist'], label='Train Negative Distance', linewidth=2, marker='s')\n",
    "axes[1].plot(epochs_range, history['val_pos_dist'], label='Val Positive Distance', linewidth=2, marker='^', linestyle='--')\n",
    "axes[1].plot(epochs_range, history['val_neg_dist'], label='Val Negative Distance', linewidth=2, marker='v', linestyle='--')\n",
    "axes[1].set_title('Embedding Distances', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Distance')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Training Loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"\\nFinal Positive Distance: {history['val_pos_dist'][-1]:.4f}\")\n",
    "print(f\"Final Negative Distance: {history['val_neg_dist'][-1]:.4f}\")\n",
    "print(f\"Distance Margin: {history['val_neg_dist'][-1] - history['val_pos_dist'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7156a11",
   "metadata": {},
   "source": [
    "## 9. Generate Embeddings for All Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36097709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model, image_path, transform, device):\n",
    "    \"\"\"Get embedding for a single image\"\"\"\n",
    "    model.eval()\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = model(img_tensor)\n",
    "    \n",
    "    return embedding.cpu().numpy()[0]\n",
    "\n",
    "# Generate embeddings for all images\n",
    "print(\"Generating embeddings for all images...\")\n",
    "embeddings_data = []\n",
    "\n",
    "for img_info in tqdm(all_images, desc='Computing embeddings'):\n",
    "    embedding = get_embedding(model, img_info['path'], val_transform, DEVICE)\n",
    "    embeddings_data.append({\n",
    "        'path': str(img_info['path']),\n",
    "        'category': img_info['category'],\n",
    "        'embedding': embedding\n",
    "    })\n",
    "\n",
    "print(f\"\\n✓ Generated {len(embeddings_data)} embeddings\")\n",
    "print(f\"Embedding dimension: {embeddings_data[0]['embedding'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b04a485",
   "metadata": {},
   "source": [
    "## 10. Visualize Embeddings with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc23e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for t-SNE\n",
    "embeddings_array = np.array([item['embedding'] for item in embeddings_data])\n",
    "categories_list = [item['category'] for item in embeddings_data]\n",
    "\n",
    "# Reduce to 2D using t-SNE\n",
    "print(\"Running t-SNE dimensionality reduction...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "embeddings_2d = tsne.fit_transform(embeddings_array)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 10))\n",
    "colors = plt.cm.tab10(range(len(categories)))\n",
    "category_to_color = {cat: colors[i] for i, cat in enumerate(categories)}\n",
    "\n",
    "for category in categories:\n",
    "    mask = np.array([cat == category for cat in categories_list])\n",
    "    plt.scatter(\n",
    "        embeddings_2d[mask, 0],\n",
    "        embeddings_2d[mask, 1],\n",
    "        c=[category_to_color[category]],\n",
    "        label=category,\n",
    "        alpha=0.7,\n",
    "        s=100,\n",
    "        edgecolors='black',\n",
    "        linewidth=0.5\n",
    "    )\n",
    "\n",
    "plt.title('t-SNE Visualization of Jewelry Embeddings', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Embeddings visualized!\")\n",
    "print(\"Similar items should cluster together in the plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd2e317",
   "metadata": {},
   "source": [
    "## 11. Similarity Search: Find Similar Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc504c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_items(query_embedding, embeddings_data, top_k=5, exclude_query=True):\n",
    "    \"\"\"Find k most similar items to query embedding\"\"\"\n",
    "    query_emb = query_embedding.reshape(1, -1)\n",
    "    \n",
    "    # Compute distances\n",
    "    distances = []\n",
    "    for i, item in enumerate(embeddings_data):\n",
    "        dist = np.linalg.norm(query_emb - item['embedding'])\n",
    "        distances.append((i, dist, item))\n",
    "    \n",
    "    # Sort by distance\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    \n",
    "    # Exclude query itself if needed\n",
    "    if exclude_query:\n",
    "        results = distances[1:top_k+1]\n",
    "    else:\n",
    "        results = distances[:top_k]\n",
    "    \n",
    "    return results\n",
    "\n",
    "def visualize_similar_items(query_idx, embeddings_data, top_k=5):\n",
    "    \"\"\"Visualize query image and its most similar items\"\"\"\n",
    "    query_item = embeddings_data[query_idx]\n",
    "    query_emb = query_item['embedding']\n",
    "    \n",
    "    similar_items = find_similar_items(query_emb, embeddings_data, top_k=top_k)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, top_k + 1, figsize=(20, 4))\n",
    "    \n",
    "    # Query image\n",
    "    query_img = Image.open(query_item['path'])\n",
    "    axes[0].imshow(query_img)\n",
    "    axes[0].set_title(f\"QUERY\\n{query_item['category']}\", \n",
    "                     fontsize=12, fontweight='bold', color='blue')\n",
    "    axes[0].axis('off')\n",
    "    axes[0].set_xlabel('Query Image', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Similar images\n",
    "    for i, (idx, distance, item) in enumerate(similar_items, 1):\n",
    "        img = Image.open(item['path'])\n",
    "        axes[i].imshow(img)\n",
    "        \n",
    "        # Color code: green if same category, red if different\n",
    "        color = 'green' if item['category'] == query_item['category'] else 'red'\n",
    "        axes[i].set_title(f\"{item['category']}\\nDist: {distance:.3f}\",\n",
    "                         fontsize=10, color=color)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_xlabel(f'Rank #{i}', fontsize=9)\n",
    "    \n",
    "    plt.suptitle('Image Similarity Search Results (Green=Same Category, Red=Different)', \n",
    "                fontsize=14, fontweight='bold', y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Similarity search functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d01c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Find similar items for random queries\n",
    "print(\"Running similarity search demos...\\n\")\n",
    "\n",
    "for i in range(3):\n",
    "    random_idx = np.random.randint(0, len(embeddings_data))\n",
    "    print(f\"Demo {i+1}: Query from category '{embeddings_data[random_idx]['category']}'\")\n",
    "    visualize_similar_items(random_idx, embeddings_data, top_k=5)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd29bc0",
   "metadata": {},
   "source": [
    "## 12. Evaluation: Retrieval Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9154d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(embeddings_data, k_values=[1, 3, 5, 10]):\n",
    "    \"\"\"Evaluate retrieval performance using Precision@K\"\"\"\n",
    "    results = {k: [] for k in k_values}\n",
    "    \n",
    "    for query_idx, query_item in enumerate(tqdm(embeddings_data, desc='Evaluating')):\n",
    "        query_category = query_item['category']\n",
    "        query_emb = query_item['embedding']\n",
    "        \n",
    "        # Find similar items\n",
    "        for k in k_values:\n",
    "            similar = find_similar_items(query_emb, embeddings_data, top_k=k, exclude_query=True)\n",
    "            \n",
    "            # Calculate precision: how many of top-k are same category?\n",
    "            correct = sum(1 for _, _, item in similar if item['category'] == query_category)\n",
    "            precision = correct / k\n",
    "            results[k].append(precision)\n",
    "    \n",
    "    # Average precision\n",
    "    avg_precision = {k: np.mean(results[k]) for k in k_values}\n",
    "    \n",
    "    return avg_precision\n",
    "\n",
    "# Evaluate\n",
    "print(\"Evaluating retrieval performance...\\n\")\n",
    "precision_at_k = evaluate_retrieval(embeddings_data, k_values=[1, 3, 5, 10])\n",
    "\n",
    "print(\"\\nRetrieval Performance (Precision@K):\")\n",
    "print(\"=\" * 40)\n",
    "for k, prec in precision_at_k.items():\n",
    "    print(f\"Precision@{k}: {prec:.4f} ({prec*100:.2f}%)\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "k_vals = list(precision_at_k.keys())\n",
    "prec_vals = list(precision_at_k.values())\n",
    "plt.bar(k_vals, prec_vals, color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "plt.xlabel('K (Number of Retrieved Items)', fontsize=12)\n",
    "plt.ylabel('Precision@K', fontsize=12)\n",
    "plt.title('Retrieval Performance', fontsize=14, fontweight='bold')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "for i, (k, prec) in enumerate(precision_at_k.items()):\n",
    "    plt.text(i, prec + 0.02, f'{prec:.3f}', ha='center', fontsize=10, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae50840",
   "metadata": {},
   "source": [
    "## 13. Save Model and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de949f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "# Save model\n",
    "final_model_path = MODEL_DIR / 'jewelry_similarity_final.pth'\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"✓ Model saved to: {final_model_path}\")\n",
    "\n",
    "# Save embeddings\n",
    "embeddings_path = MODEL_DIR / 'jewelry_embeddings.pkl'\n",
    "with open(embeddings_path, 'wb') as f:\n",
    "    pickle.dump(embeddings_data, f)\n",
    "print(f\"✓ Embeddings saved to: {embeddings_path}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'embedding_dim': EMBEDDING_DIM,\n",
    "    'img_size': IMG_SIZE,\n",
    "    'categories': categories,\n",
    "    'num_images': len(embeddings_data),\n",
    "    'precision_at_k': {str(k): float(v) for k, v in precision_at_k.items()}\n",
    "}\n",
    "metadata_path = MODEL_DIR / 'similarity_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"✓ Metadata saved to: {metadata_path}\")\n",
    "\n",
    "# Model summary\n",
    "model_size_mb = os.path.getsize(final_model_path) / (1024 * 1024)\n",
    "embeddings_size_mb = os.path.getsize(embeddings_path) / (1024 * 1024)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"MODEL SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Task: Image Similarity (Siamese Network)\")\n",
    "print(f\"Embedding dimension: {EMBEDDING_DIM}\")\n",
    "print(f\"Number of images: {len(embeddings_data)}\")\n",
    "print(f\"Categories: {len(categories)}\")\n",
    "print(f\"Model size: {model_size_mb:.2f} MB\")\n",
    "print(f\"Embeddings size: {embeddings_size_mb:.2f} MB\")\n",
    "print(f\"Precision@5: {precision_at_k[5]:.4f}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fe7ab4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated image similarity learning for jewelry:\n",
    "\n",
    "1. **Data Preparation**: Created triplet dataset (anchor, positive, negative)\n",
    "2. **Model Architecture**: Siamese network with MobileNetV2 backbone\n",
    "3. **Training**: Triplet loss to learn discriminative embeddings\n",
    "4. **Embeddings**: Generated 128-D vectors for all images\n",
    "5. **Visualization**: t-SNE plot showing clustering by category\n",
    "6. **Similarity Search**: Find visually similar items using distance metrics\n",
    "7. **Evaluation**: Precision@K metrics for retrieval quality\n",
    "\n",
    "**Key Differences from Classification:**\n",
    "- No fixed number of classes - can add new items without retraining\n",
    "- Captures visual similarity beyond category labels\n",
    "- Useful for recommendation systems\n",
    "- Can handle new jewelry types not seen during training\n",
    "\n",
    "**Use Cases:**\n",
    "- \"Find similar items\" in e-commerce\n",
    "- Visual search engines\n",
    "- Duplicate detection\n",
    "- Product recommendations\n",
    "- Clustering and organization"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
