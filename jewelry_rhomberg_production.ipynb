{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rhomberg Jewelry Multi-Label Classification\n",
    "\n",
    "Train a multi-label classifier on real Rhomberg jewelry data.\n",
    "\n",
    "## Features:\n",
    "- ‚úÖ **Configurable download**: Choose how many images to process\n",
    "- ‚úÖ **Smart access**: Auto-detects local vs remote execution  \n",
    "- ‚úÖ **HTTP download**: Fast image downloads (vs SSH)\n",
    "- ‚úÖ **Caching**: Skips already downloaded images\n",
    "- ‚úÖ **Multi-label**: Category, Gender, Material, Price Range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration - ADJUST THESE SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuration loaded\n",
      "  Images per category: 10\n",
      "  Training epochs: 20\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DATASET CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# How many images per category?\n",
    "MAX_IMAGES_PER_CATEGORY = 10  # Options:\n",
    "#   200  = ~1,400 images (fast testing, ~5-10 min download)\n",
    "#   500  = ~3,500 images (medium, ~15-25 min download)\n",
    "#   None = ALL 12,026 images (full dataset, ~1-2 hours download)\n",
    "\n",
    "# ============================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# ============================================================\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# ============================================================\n",
    "# PATHS\n",
    "# ============================================================\n",
    "CSV_PATH = \"/project/data/jewlery.csv\"\n",
    "OUTPUT_DIR = \"/project/data/rhomberg_final\"  # Persists on host\n",
    "MODEL_DIR = \"/project/models\"                 # Persists on host\n",
    "\n",
    "# NOTE: Images ARE tracked by git in AI Studio\n",
    "# To avoid this, either:\n",
    "# 1. Add 'data/rhomberg_final/' to .gitignore (done)\n",
    "# 2. Don't commit these files to git\n",
    "# 3. Use git-lfs for large files\n",
    "\n",
    "print(\"‚úì Configuration loaded\")\n",
    "print(f\"  Images per category: {MAX_IMAGES_PER_CATEGORY if MAX_IMAGES_PER_CATEGORY else 'ALL'}\")\n",
    "print(f\"  Training epochs: {EPOCHS}\")\n",
    "print(f\"‚ö†Ô∏è  Images in git-tracked folder - add to .gitignore!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.6.0a0+df5bbc09d1.nv24.11\n",
      "Device: cpu\n",
      "CUDA: False\n",
      "‚úì Imports complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create directories\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True, parents=True)\n",
    "Path(MODEL_DIR).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"‚úì Imports complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download/Access Images\n",
    "\n",
    "**This will:**\n",
    "- Auto-detect if on spark (local files) or remote (HTTP download)\n",
    "- Process CSV and extract metadata\n",
    "- Download/copy images with caching\n",
    "- Create training-ready dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROCESSING RHOMBERG DATA\n",
      "======================================================================\n",
      "\n",
      "üåê REMOTE ‚Üí HTTP download\n",
      "Total products: 12,026\n",
      "\n",
      "Sampling 200 per category...\n",
      "  rings          :  200\n",
      "  earrings       :  200\n",
      "  pendants       :  200\n",
      "  necklaces      :  200\n",
      "  bracelets      :  200\n",
      "  fussketten     :  153\n",
      "  accessoires    :   56\n",
      "  piercing       :  200\n",
      "  diverses       :   24\n",
      "\n",
      "Processing 1,433 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|‚ñà‚ñà        | 298/1433 [01:33<05:46,  3.27it/s]"
     ]
    }
   ],
   "source": [
    "# Helper functions\n",
    "def is_on_spark():\n",
    "    return Path('/mnt/img/jpeg/detailbilder').exists()\n",
    "\n",
    "def clean_category(product_type):\n",
    "    if pd.isna(product_type): return 'unknown'\n",
    "    parts = str(product_type).split('>')\n",
    "    if len(parts) > 0:\n",
    "        main = parts[0].strip().lower()\n",
    "        mapping = {'fingerringe': 'rings', 'ohrschmuck': 'earrings', 'halsschmuck': 'necklaces',\n",
    "                   'armschmuck': 'bracelets', 'anh√§nger': 'pendants', 'piercing': 'piercing', 'fu√üketten': 'anklets'}\n",
    "        for de, en in mapping.items():\n",
    "            if de in main: return en\n",
    "        return main.split()[0] if main else 'unknown'\n",
    "    return 'unknown'\n",
    "\n",
    "def extract_material(material_str):\n",
    "    if pd.isna(material_str): return 'unknown'\n",
    "    material = str(material_str).lower()\n",
    "    if 'platin' in material: return 'platinum'\n",
    "    elif 'gold' in material: return 'gold'\n",
    "    elif 'silber' in material or 'silver' in material: return 'silver'\n",
    "    elif 'edelstahl' in material or 'stainless' in material: return 'stainless_steel'\n",
    "    elif 'titan' in material: return 'titan'\n",
    "    else: return material.split()[0] if material.split() else 'unknown'\n",
    "\n",
    "def download_image_http(url, dest_path):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        with open(dest_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def copy_local_image(product_id, dest_path):\n",
    "    try:\n",
    "        source_path = Path('/mnt/img/jpeg/detailbilder') / '360' / f\"{product_id}.jpg\"\n",
    "        if source_path.exists():\n",
    "            shutil.copy2(source_path, dest_path)\n",
    "            return True\n",
    "        return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Process data\n",
    "print(\"=\"*70)\n",
    "print(\"PROCESSING RHOMBERG DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "on_spark = is_on_spark()\n",
    "print(f\"\\n{'üéØ ON spark' if on_spark else 'üåê REMOTE'} ‚Üí {'Local files' if on_spark else 'HTTP download'}\")\n",
    "\n",
    "df_raw = pd.read_csv(CSV_PATH, sep='\\t', on_bad_lines='skip')\n",
    "print(f\"Total products: {len(df_raw):,}\")\n",
    "\n",
    "df_raw['category'] = df_raw['product_type'].apply(clean_category)\n",
    "df_raw['material_clean'] = df_raw['material'].apply(extract_material)\n",
    "df_raw['gender_clean'] = df_raw['gender'].fillna('unisex')\n",
    "df_raw['price_clean'] = df_raw['price'].str.replace(' EUR', '').str.replace(',', '.').astype(float, errors='ignore')\n",
    "df_raw['price_range'] = df_raw['price_clean'].apply(lambda p: 'budget' if p < 50 else ('mid_range' if p < 100 else ('premium' if p < 300 else 'luxury')) if not pd.isna(p) else 'unknown')\n",
    "\n",
    "df_with_images = df_raw[df_raw['image_link'].notna()].copy()\n",
    "\n",
    "if MAX_IMAGES_PER_CATEGORY:\n",
    "    print(f\"\\nSampling {MAX_IMAGES_PER_CATEGORY} per category...\")\n",
    "    sampled = []\n",
    "    for cat in df_with_images['category'].unique():\n",
    "        cat_df = df_with_images[df_with_images['category'] == cat]\n",
    "        n = min(MAX_IMAGES_PER_CATEGORY, len(cat_df))\n",
    "        sampled.append(cat_df.sample(n=n, random_state=42))\n",
    "        print(f\"  {cat:15s}: {n:4d}\")\n",
    "    df_to_process = pd.concat(sampled, ignore_index=True)\n",
    "else:\n",
    "    df_to_process = df_with_images\n",
    "\n",
    "print(f\"\\nProcessing {len(df_to_process):,} images...\")\n",
    "\n",
    "images_dir = Path(OUTPUT_DIR) / 'images'\n",
    "images_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "results = []\n",
    "found, not_found, skipped = 0, 0, 0\n",
    "\n",
    "for _, row in tqdm(df_to_process.iterrows(), total=len(df_to_process)):\n",
    "    cat, pid = row['category'], row['id']\n",
    "    cat_dir = images_dir / cat\n",
    "    cat_dir.mkdir(exist_ok=True)\n",
    "    dest = cat_dir / f\"{cat}_{pid}.jpg\"\n",
    "    \n",
    "    if dest.exists():\n",
    "        skipped += 1\n",
    "        found += 1\n",
    "    elif (copy_local_image(pid, dest) if on_spark else download_image_http(row['image_link'], dest)):\n",
    "        found += 1\n",
    "    else:\n",
    "        not_found += 1\n",
    "        continue\n",
    "    \n",
    "    results.append({\n",
    "        'filename': f\"{cat}_{pid}.jpg\",\n",
    "        'filepath': str(dest),\n",
    "        'product_id': pid,\n",
    "        'title': row['title'],\n",
    "        'category': cat,\n",
    "        'gender': row['gender_clean'],\n",
    "        'material': row['material_clean'],\n",
    "        'price': row['price_clean'],\n",
    "        'price_range': row['price_range']\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "metadata_path = Path(OUTPUT_DIR) / 'jewelry_metadata.csv'\n",
    "df.to_csv(metadata_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úì Processed: {found:,} | ‚ö° Cached: {skipped:,} | ‚úó Failed: {not_found:,}\")\n",
    "print(f\"‚úì Saved: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Dataset\n",
    "\n",
    "Check the distribution of categories, materials, gender, and price ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(f\"Total images: {len(df):,}\\n\")\n",
    "\n",
    "print(\"CATEGORY DISTRIBUTION:\")\n",
    "for cat, count in df['category'].value_counts().items():\n",
    "    print(f\"  {cat:15s}: {count:4d} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nGENDER DISTRIBUTION:\")\n",
    "for gender, count in df['gender'].value_counts().items():\n",
    "    print(f\"  {gender:15s}: {count:4d} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nMATERIAL DISTRIBUTION:\")\n",
    "for mat, count in df['material'].value_counts().items():\n",
    "    print(f\"  {mat:15s}: {count:4d} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nPRICE RANGE DISTRIBUTION:\")\n",
    "for pr, count in df['price_range'].value_counts().items():\n",
    "    print(f\"  {pr:15s}: {count:4d} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Create label encodings\n",
    "category_labels = {cat: idx for idx, cat in enumerate(sorted(df['category'].unique()))}\n",
    "gender_labels = {g: idx for idx, g in enumerate(sorted(df['gender'].unique()))}\n",
    "material_labels = {m: idx for idx, m in enumerate(sorted(df['material'].unique()))}\n",
    "price_labels = {p: idx for idx, p in enumerate(sorted(df['price_range'].unique()))}\n",
    "\n",
    "print(f\"\\n‚úì Label encodings created\")\n",
    "print(f\"  Categories: {len(category_labels)}\")\n",
    "print(f\"  Genders: {len(gender_labels)}\")\n",
    "print(f\"  Materials: {len(material_labels)}\")\n",
    "print(f\"  Price ranges: {len(price_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Class\n",
    "\n",
    "Custom PyTorch Dataset with multi-label support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelJewelryDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = row['filepath']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Create labels\n",
    "        labels = {\n",
    "            'category': category_labels[row['category']],\n",
    "            'gender': gender_labels[row['gender']],\n",
    "            'material': material_labels[row['material']],\n",
    "            'price_range': price_labels[row['price_range']]\n",
    "        }\n",
    "        \n",
    "        return image, labels\n",
    "\n",
    "# Create transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Split data\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['category'])\n",
    "\n",
    "train_dataset = MultiLabelJewelryDataset(train_df, transform=train_transform)\n",
    "val_dataset = MultiLabelJewelryDataset(val_df, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"‚úì Datasets created\")\n",
    "print(f\"  Train: {len(train_dataset):,} images\")\n",
    "print(f\"  Val: {len(val_dataset):,} images\")\n",
    "print(f\"  Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Definition\n",
    "\n",
    "Multi-head classifier based on MobileNetV2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelJewelryModel(nn.Module):\n",
    "    def __init__(self, num_categories, num_genders, num_materials, num_price_ranges):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained MobileNetV2\n",
    "        mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # Extract features (remove classifier)\n",
    "        self.features = mobilenet.features\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Shared layer\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1280, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Task-specific heads\n",
    "        self.head_category = nn.Linear(512, num_categories)\n",
    "        self.head_gender = nn.Linear(512, num_genders)\n",
    "        self.head_material = nn.Linear(512, num_materials)\n",
    "        self.head_price_range = nn.Linear(512, num_price_ranges)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.shared(x)\n",
    "        \n",
    "        return {\n",
    "            'category': self.head_category(x),\n",
    "            'gender': self.head_gender(x),\n",
    "            'material': self.head_material(x),\n",
    "            'price_range': self.head_price_range(x)\n",
    "        }\n",
    "\n",
    "# Create model\n",
    "model = MultiLabelJewelryModel(\n",
    "    num_categories=len(category_labels),\n",
    "    num_genders=len(gender_labels),\n",
    "    num_materials=len(material_labels),\n",
    "    num_price_ranges=len(price_labels)\n",
    ").to(DEVICE)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"‚úì Model created\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop\n",
    "\n",
    "Train the model with multi-task learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = {task: 0 for task in ['category', 'gender', 'material', 'price_range']}\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = {k: v.to(device) for k, v in labels.items()}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate loss for each task\n",
    "        loss = sum(criterion(outputs[task], labels[task]) for task in outputs.keys())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total += images.size(0)\n",
    "        \n",
    "        # Calculate accuracy for each task\n",
    "        for task in outputs.keys():\n",
    "            _, predicted = outputs[task].max(1)\n",
    "            correct[task] += predicted.eq(labels[task]).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracies = {task: 100. * correct[task] / total for task in correct.keys()}\n",
    "    \n",
    "    return avg_loss, accuracies\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = {task: 0 for task in ['category', 'gender', 'material', 'price_range']}\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc='Validation'):\n",
    "            images = images.to(device)\n",
    "            labels = {k: v.to(device) for k, v in labels.items()}\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = sum(criterion(outputs[task], labels[task]) for task in outputs.keys())\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total += images.size(0)\n",
    "            \n",
    "            for task in outputs.keys():\n",
    "                _, predicted = outputs[task].max(1)\n",
    "                correct[task] += predicted.eq(labels[task]).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracies = {task: 100. * correct[task] / total for task in correct.keys()}\n",
    "    \n",
    "    return avg_loss, accuracies\n",
    "\n",
    "# Training loop\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TRAINING\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, DEVICE)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"\\nTrain Loss: {train_loss:.4f}\")\n",
    "    print(f\"Train Acc - Category: {train_acc['category']:.2f}%, Gender: {train_acc['gender']:.2f}%, \"\n",
    "          f\"Material: {train_acc['material']:.2f}%, Price: {train_acc['price_range']:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Val Acc - Category: {val_acc['category']:.2f}%, Gender: {val_acc['gender']:.2f}%, \"\n",
    "          f\"Material: {val_acc['material']:.2f}%, Price: {val_acc['price_range']:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), Path(MODEL_DIR) / 'best_multilabel_model.pth')\n",
    "        print(\"‚úì Saved best model\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Results\n",
    "\n",
    "Plot training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', marker='o')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot accuracy for each task\n",
    "tasks = ['category', 'gender', 'material', 'price_range']\n",
    "for task in tasks:\n",
    "    train_accs = [epoch_acc[task] for epoch_acc in history['train_acc']]\n",
    "    val_accs = [epoch_acc[task] for epoch_acc in history['val_acc']]\n",
    "    axes[1].plot(val_accs, label=task.replace('_', ' ').title(), marker='o')\n",
    "\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Validation Accuracy by Task')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(MODEL_DIR) / 'training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Training curves saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Predictions\n",
    "\n",
    "Visualize predictions on sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reverse mappings\n",
    "category_names = {v: k for k, v in category_labels.items()}\n",
    "gender_names = {v: k for k, v in gender_labels.items()}\n",
    "material_names = {v: k for k, v in material_labels.items()}\n",
    "price_names = {v: k for k, v in price_labels.items()}\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(Path(MODEL_DIR) / 'best_multilabel_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Get some samples\n",
    "sample_indices = np.random.choice(len(val_dataset), 8, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, sample_idx in enumerate(sample_indices):\n",
    "    image, labels = val_dataset[sample_idx]\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        image_batch = image.unsqueeze(0).to(DEVICE)\n",
    "        outputs = model(image_batch)\n",
    "        \n",
    "        predictions = {\n",
    "            'category': category_names[outputs['category'].argmax(1).item()],\n",
    "            'gender': gender_names[outputs['gender'].argmax(1).item()],\n",
    "            'material': material_names[outputs['material'].argmax(1).item()],\n",
    "            'price_range': price_names[outputs['price_range'].argmax(1).item()]\n",
    "        }\n",
    "        \n",
    "        true_labels = {\n",
    "            'category': category_names[labels['category']],\n",
    "            'gender': gender_names[labels['gender']],\n",
    "            'material': material_names[labels['material']],\n",
    "            'price_range': price_names[labels['price_range']]\n",
    "        }\n",
    "    \n",
    "    # Display\n",
    "    img_path = val_df.iloc[sample_idx]['filepath']\n",
    "    img = Image.open(img_path)\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].axis('off')\n",
    "    \n",
    "    title = f\"True: {true_labels['category']}\\n\"\n",
    "    title += f\"Pred: {predictions['category']}\\n\"\n",
    "    title += f\"Mat: {predictions['material']} | {predictions['gender']}\"\n",
    "    \n",
    "    color = 'green' if predictions['category'] == true_labels['category'] else 'red'\n",
    "    axes[idx].set_title(title, fontsize=9, color=color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(MODEL_DIR) / 'predictions_sample.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Sample predictions visualized\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
