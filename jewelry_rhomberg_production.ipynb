{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rhomberg Jewelry Multi-Label Classification\n",
    "\n",
    "Train a multi-label classifier on real Rhomberg jewelry data.\n",
    "\n",
    "## Features:\n",
    "- ‚úÖ **Configurable download**: Choose how many images to process\n",
    "- ‚úÖ **Smart access**: Auto-detects local vs remote execution  \n",
    "- ‚úÖ **HTTP download**: Fast image downloads (vs SSH)\n",
    "- ‚úÖ **Caching**: Skips already downloaded images\n",
    "- ‚úÖ **Multi-label**: Category, Gender, Material, Price Range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration - ADJUST THESE SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuration loaded\n",
      "  Images per category: ALL\n",
      "  Training epochs: 20\n",
      "‚ö†Ô∏è  Images in git-tracked folder - add to .gitignore!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DATASET CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# How many images per category?\n",
    "MAX_IMAGES_PER_CATEGORY = None  # Options:\n",
    "#   200  = ~1,400 images (fast testing, ~5-10 min download)\n",
    "#   500  = ~3,500 images (medium, ~15-25 min download)\n",
    "#   None = ALL 12,026 images (full dataset, ~1-2 hours download)\n",
    "\n",
    "# ============================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# ============================================================\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# ============================================================\n",
    "# PATHS\n",
    "# ============================================================\n",
    "CSV_PATH = \"/project/data/jewlery.csv\"\n",
    "OUTPUT_DIR = \"/project/data/rhomberg_final\"  # Persists on host\n",
    "MODEL_DIR = \"/project/models\"                 # Persists on host\n",
    "\n",
    "# NOTE: Images ARE tracked by git in AI Studio\n",
    "# To avoid this, either:\n",
    "# 1. Add 'data/rhomberg_final/' to .gitignore (done)\n",
    "# 2. Don't commit these files to git\n",
    "# 3. Use git-lfs for large files\n",
    "\n",
    "print(\"‚úì Configuration loaded\")\n",
    "print(f\"  Images per category: {MAX_IMAGES_PER_CATEGORY if MAX_IMAGES_PER_CATEGORY else 'ALL'}\")\n",
    "print(f\"  Training epochs: {EPOCHS}\")\n",
    "print(f\"‚ö†Ô∏è  Images in git-tracked folder - add to .gitignore!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.6.0a0+df5bbc09d1.nv24.11\n",
      "Device: cuda\n",
      "CUDA: True\n",
      "GPU: NVIDIA GB10\n",
      "‚úì Imports complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create directories\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True, parents=True)\n",
    "Path(MODEL_DIR).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"‚úì Imports complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download/Access Images\n",
    "\n",
    "**This will:**\n",
    "- Auto-detect if on spark (local files) or remote (HTTP download)\n",
    "- Process CSV and extract metadata\n",
    "- Download/copy images with caching\n",
    "- Create training-ready dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROCESSING RHOMBERG DATA\n",
      "======================================================================\n",
      "\n",
      "üéØ ON spark ‚Üí Local files\n",
      "Total products: 12,026\n",
      "\n",
      "Processing 12,026 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12026/12026 [00:07<00:00, 1677.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Processed: 46 | ‚ö° Cached: 46 | ‚úó Failed: 11,980\n",
      "‚úì Saved: /project/data/rhomberg_final/jewelry_metadata.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Helper functions\n",
    "def is_on_spark():\n",
    "    return Path('/mnt/img/jpeg/detailbilder').exists()\n",
    "\n",
    "def clean_category(product_type):\n",
    "    if pd.isna(product_type): return 'unknown'\n",
    "    parts = str(product_type).split('>')\n",
    "    if len(parts) > 0:\n",
    "        main = parts[0].strip().lower()\n",
    "        mapping = {'fingerringe': 'rings', 'ohrschmuck': 'earrings', 'halsschmuck': 'necklaces',\n",
    "                   'armschmuck': 'bracelets', 'anh√§nger': 'pendants', 'piercing': 'piercing', 'fu√üketten': 'anklets'}\n",
    "        for de, en in mapping.items():\n",
    "            if de in main: return en\n",
    "        return main.split()[0] if main else 'unknown'\n",
    "    return 'unknown'\n",
    "\n",
    "def extract_material(material_str):\n",
    "    \"\"\"\n",
    "    Extract and normalize material names.\n",
    "    Simply takes the first word and lowercases it.\n",
    "    This automatically handles all materials: Silber, Gold, Edelstahl, Titan, \n",
    "    Platin, Wolfram, Tantal, Keramik, Palladium, etc.\n",
    "    \"\"\"\n",
    "    if pd.isna(material_str): \n",
    "        return 'unknown'\n",
    "    \n",
    "    material = str(material_str).strip()\n",
    "    if not material:\n",
    "        return 'unknown'\n",
    "    \n",
    "    # Get first word and normalize to lowercase\n",
    "    first_word = material.split()[0].lower()\n",
    "    return first_word\n",
    "\n",
    "def download_image_http(url, dest_path):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        with open(dest_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def copy_local_image(product_id, dest_path):\n",
    "    try:\n",
    "        source_path = Path('/mnt/img/jpeg/detailbilder') / '360' / f\"{product_id}.jpg\"\n",
    "        if source_path.exists():\n",
    "            shutil.copy2(source_path, dest_path)\n",
    "            return True\n",
    "        return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Process data\n",
    "print(\"=\"*70)\n",
    "print(\"PROCESSING RHOMBERG DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "on_spark = is_on_spark()\n",
    "print(f\"\\n{'üéØ ON spark' if on_spark else 'üåê REMOTE'} ‚Üí {'Local files' if on_spark else 'HTTP download'}\")\n",
    "\n",
    "df_raw = pd.read_csv(CSV_PATH, sep='\\t', on_bad_lines='skip')\n",
    "print(f\"Total products: {len(df_raw):,}\")\n",
    "\n",
    "df_raw['category'] = df_raw['product_type'].apply(clean_category)\n",
    "df_raw['material_clean'] = df_raw['material'].apply(extract_material)\n",
    "df_raw['gender_clean'] = df_raw['gender'].fillna('unisex')\n",
    "df_raw['price_clean'] = df_raw['price'].str.replace(' EUR', '').str.replace(',', '.').astype(float, errors='ignore')\n",
    "df_raw['price_range'] = df_raw['price_clean'].apply(lambda p: 'budget' if p < 50 else ('mid_range' if p < 100 else ('premium' if p < 300 else 'luxury')) if not pd.isna(p) else 'unknown')\n",
    "\n",
    "df_with_images = df_raw[df_raw['image_link'].notna()].copy()\n",
    "\n",
    "if MAX_IMAGES_PER_CATEGORY:\n",
    "    print(f\"\\nSampling {MAX_IMAGES_PER_CATEGORY} per category...\")\n",
    "    sampled = []\n",
    "    for cat in df_with_images['category'].unique():\n",
    "        cat_df = df_with_images[df_with_images['category'] == cat]\n",
    "        n = min(MAX_IMAGES_PER_CATEGORY, len(cat_df))\n",
    "        sampled.append(cat_df.sample(n=n, random_state=42))\n",
    "        print(f\"  {cat:15s}: {n:4d}\")\n",
    "    df_to_process = pd.concat(sampled, ignore_index=True)\n",
    "else:\n",
    "    df_to_process = df_with_images\n",
    "\n",
    "print(f\"\\nProcessing {len(df_to_process):,} images...\")\n",
    "\n",
    "images_dir = Path(OUTPUT_DIR) / 'images'\n",
    "images_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "results = []\n",
    "found, not_found, skipped = 0, 0, 0\n",
    "\n",
    "for _, row in tqdm(df_to_process.iterrows(), total=len(df_to_process)):\n",
    "    cat, pid = row['category'], row['id']\n",
    "    cat_dir = images_dir / cat\n",
    "    cat_dir.mkdir(exist_ok=True)\n",
    "    dest = cat_dir / f\"{cat}_{pid}.jpg\"\n",
    "    \n",
    "    if dest.exists():\n",
    "        skipped += 1\n",
    "        found += 1\n",
    "    elif (copy_local_image(pid, dest) if on_spark else download_image_http(row['image_link'], dest)):\n",
    "        found += 1\n",
    "    else:\n",
    "        not_found += 1\n",
    "        continue\n",
    "    \n",
    "    results.append({\n",
    "        'filename': f\"{cat}_{pid}.jpg\",\n",
    "        'filepath': str(dest),\n",
    "        'product_id': pid,\n",
    "        'title': row['title'],\n",
    "        'category': cat,\n",
    "        'gender': row['gender_clean'],\n",
    "        'material': row['material_clean'],\n",
    "        'price': row['price_clean'],\n",
    "        'price_range': row['price_range']\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "metadata_path = Path(OUTPUT_DIR) / 'jewelry_metadata.csv'\n",
    "df.to_csv(metadata_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úì Processed: {found:,} | ‚ö° Cached: {skipped:,} | ‚úó Failed: {not_found:,}\")\n",
    "print(f\"‚úì Saved: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Dataset\n",
    "\n",
    "Check the distribution of categories, materials, gender, and price ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATASET STATISTICS\n",
      "======================================================================\n",
      "\n",
      "Total images: 46\n",
      "\n",
      "CATEGORY DISTRIBUTION:\n",
      "  bracelets      :   10 (21.7%)\n",
      "  pendants       :   10 (21.7%)\n",
      "  rings          :   10 (21.7%)\n",
      "  necklaces      :    9 (19.6%)\n",
      "  earrings       :    6 (13.0%)\n",
      "  fussketten     :    1 (2.2%)\n",
      "\n",
      "GENDER DISTRIBUTION:\n",
      "  female         :   33 (71.7%)\n",
      "  male           :    8 (17.4%)\n",
      "  unisex         :    5 (10.9%)\n",
      "\n",
      "MATERIAL DISTRIBUTION:\n",
      "  silver         :   27 (58.7%)\n",
      "  stainless_steel:    9 (19.6%)\n",
      "  gold           :    7 (15.2%)\n",
      "  unknown        :    2 (4.3%)\n",
      "  titan          :    1 (2.2%)\n",
      "\n",
      "PRICE RANGE DISTRIBUTION:\n",
      "  budget         :   17 (37.0%)\n",
      "  mid_range      :   16 (34.8%)\n",
      "  premium        :    9 (19.6%)\n",
      "  luxury         :    4 (8.7%)\n",
      "\n",
      "‚úì Label encodings created\n",
      "  Categories: 6\n",
      "  Genders: 3\n",
      "  Materials: 5\n",
      "  Price ranges: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(f\"Total images: {len(df):,}\\n\")\n",
    "\n",
    "print(\"CATEGORY DISTRIBUTION:\")\n",
    "for cat, count in df['category'].value_counts().items():\n",
    "    print(f\"  {cat:15s}: {count:4d} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nGENDER DISTRIBUTION:\")\n",
    "for gender, count in df['gender'].value_counts().items():\n",
    "    print(f\"  {gender:15s}: {count:4d} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nMATERIAL DISTRIBUTION:\")\n",
    "for mat, count in df['material'].value_counts().items():\n",
    "    print(f\"  {mat:15s}: {count:4d} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nPRICE RANGE DISTRIBUTION:\")\n",
    "for pr, count in df['price_range'].value_counts().items():\n",
    "    print(f\"  {pr:15s}: {count:4d} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Create label encodings\n",
    "category_labels = {cat: idx for idx, cat in enumerate(sorted(df['category'].unique()))}\n",
    "gender_labels = {g: idx for idx, g in enumerate(sorted(df['gender'].unique()))}\n",
    "material_labels = {m: idx for idx, m in enumerate(sorted(df['material'].unique()))}\n",
    "price_labels = {p: idx for idx, p in enumerate(sorted(df['price_range'].unique()))}\n",
    "\n",
    "print(f\"\\n‚úì Label encodings created\")\n",
    "print(f\"  Categories: {len(category_labels)}\")\n",
    "print(f\"  Genders: {len(gender_labels)}\")\n",
    "print(f\"  Materials: {len(material_labels)}\")\n",
    "print(f\"  Price ranges: {len(price_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Class\n",
    "\n",
    "Custom PyTorch Dataset with multi-label support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 46\u001b[0m\n\u001b[1;32m     39\u001b[0m val_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     40\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((IMG_SIZE, IMG_SIZE)),\n\u001b[1;32m     41\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     42\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize([\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], [\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[1;32m     43\u001b[0m ])\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Split data\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m train_df, val_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m MultiLabelJewelryDataset(train_df, transform\u001b[38;5;241m=\u001b[39mtrain_transform)\n\u001b[1;32m     49\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m MultiLabelJewelryDataset(val_df, transform\u001b[38;5;241m=\u001b[39mval_transform)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py:2806\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2802\u001b[0m         CVClass \u001b[38;5;241m=\u001b[39m ShuffleSplit\n\u001b[1;32m   2804\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m-> 2806\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstratify\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2808\u001b[0m train, test \u001b[38;5;241m=\u001b[39m ensure_common_namespace_device(arrays[\u001b[38;5;241m0\u001b[39m], train, test)\n\u001b[1;32m   2810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   2811\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m   2812\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[1;32m   2813\u001b[0m     )\n\u001b[1;32m   2814\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py:1843\u001b[0m, in \u001b[0;36mBaseShuffleSplit.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[1;32m   1814\u001b[0m \n\u001b[1;32m   1815\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1840\u001b[0m \u001b[38;5;124;03mto an integer.\u001b[39;00m\n\u001b[1;32m   1841\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1842\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[0;32m-> 1843\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1844\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py:2252\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit._iter_indices\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2250\u001b[0m class_counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(y_indices)\n\u001b[1;32m   2251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmin(class_counts) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m-> 2252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2253\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe least populated class in y has only 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2254\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m member, which is too few. The minimum\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2255\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m number of groups for any class cannot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be less than 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2257\u001b[0m     )\n\u001b[1;32m   2259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m<\u001b[39m n_classes:\n\u001b[1;32m   2260\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2261\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe train_size = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m should be greater or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2262\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequal to the number of classes = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_train, n_classes)\n\u001b[1;32m   2263\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "class MultiLabelJewelryDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = row['filepath']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Create labels\n",
    "        labels = {\n",
    "            'category': category_labels[row['category']],\n",
    "            'gender': gender_labels[row['gender']],\n",
    "            'material': material_labels[row['material']],\n",
    "            'price_range': price_labels[row['price_range']]\n",
    "        }\n",
    "        \n",
    "        return image, labels\n",
    "\n",
    "# Create transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Filter out categories with less than 2 samples (required for stratified split)\n",
    "category_counts = df['category'].value_counts()\n",
    "categories_to_keep = category_counts[category_counts >= 2].index\n",
    "df_filtered = df[df['category'].isin(categories_to_keep)].copy()\n",
    "\n",
    "removed_categories = set(category_counts.index) - set(categories_to_keep)\n",
    "if removed_categories:\n",
    "    removed_count = len(df) - len(df_filtered)\n",
    "    print(f\"‚ö†Ô∏è  Removed {removed_count} image(s) from {len(removed_categories)} category/categories with <2 samples: {removed_categories}\")\n",
    "\n",
    "# Split data with stratification\n",
    "train_df, val_df = train_test_split(df_filtered, test_size=0.2, random_state=42, stratify=df_filtered['category'])\n",
    "\n",
    "train_dataset = MultiLabelJewelryDataset(train_df, transform=train_transform)\n",
    "val_dataset = MultiLabelJewelryDataset(val_df, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"‚úì Datasets created\")\n",
    "print(f\"  Train: {len(train_dataset):,} images\")\n",
    "print(f\"  Val: {len(val_dataset):,} images\")\n",
    "print(f\"  Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Definition\n",
    "\n",
    "Multi-head classifier based on MobileNetV2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelJewelryModel(nn.Module):\n",
    "    def __init__(self, num_categories, num_genders, num_materials, num_price_ranges):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained MobileNetV2\n",
    "        mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # Extract features (remove classifier)\n",
    "        self.features = mobilenet.features\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Shared layer\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1280, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Task-specific heads\n",
    "        self.head_category = nn.Linear(512, num_categories)\n",
    "        self.head_gender = nn.Linear(512, num_genders)\n",
    "        self.head_material = nn.Linear(512, num_materials)\n",
    "        self.head_price_range = nn.Linear(512, num_price_ranges)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.shared(x)\n",
    "        \n",
    "        return {\n",
    "            'category': self.head_category(x),\n",
    "            'gender': self.head_gender(x),\n",
    "            'material': self.head_material(x),\n",
    "            'price_range': self.head_price_range(x)\n",
    "        }\n",
    "\n",
    "# Create model\n",
    "model = MultiLabelJewelryModel(\n",
    "    num_categories=len(category_labels),\n",
    "    num_genders=len(gender_labels),\n",
    "    num_materials=len(material_labels),\n",
    "    num_price_ranges=len(price_labels)\n",
    ").to(DEVICE)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"‚úì Model created\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop\n",
    "\n",
    "Train the model with multi-task learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = {task: 0 for task in ['category', 'gender', 'material', 'price_range']}\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = {k: v.to(device) for k, v in labels.items()}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate loss for each task\n",
    "        loss = sum(criterion(outputs[task], labels[task]) for task in outputs.keys())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total += images.size(0)\n",
    "        \n",
    "        # Calculate accuracy for each task\n",
    "        for task in outputs.keys():\n",
    "            _, predicted = outputs[task].max(1)\n",
    "            correct[task] += predicted.eq(labels[task]).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracies = {task: 100. * correct[task] / total for task in correct.keys()}\n",
    "    \n",
    "    return avg_loss, accuracies\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = {task: 0 for task in ['category', 'gender', 'material', 'price_range']}\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc='Validation'):\n",
    "            images = images.to(device)\n",
    "            labels = {k: v.to(device) for k, v in labels.items()}\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = sum(criterion(outputs[task], labels[task]) for task in outputs.keys())\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total += images.size(0)\n",
    "            \n",
    "            for task in outputs.keys():\n",
    "                _, predicted = outputs[task].max(1)\n",
    "                correct[task] += predicted.eq(labels[task]).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracies = {task: 100. * correct[task] / total for task in correct.keys()}\n",
    "    \n",
    "    return avg_loss, accuracies\n",
    "\n",
    "# Training loop\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TRAINING\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, DEVICE)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"\\nTrain Loss: {train_loss:.4f}\")\n",
    "    print(f\"Train Acc - Category: {train_acc['category']:.2f}%, Gender: {train_acc['gender']:.2f}%, \"\n",
    "          f\"Material: {train_acc['material']:.2f}%, Price: {train_acc['price_range']:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Val Acc - Category: {val_acc['category']:.2f}%, Gender: {val_acc['gender']:.2f}%, \"\n",
    "          f\"Material: {val_acc['material']:.2f}%, Price: {val_acc['price_range']:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), Path(MODEL_DIR) / 'best_multilabel_model.pth')\n",
    "        print(\"‚úì Saved best model\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Results\n",
    "\n",
    "Plot training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', marker='o')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot accuracy for each task\n",
    "tasks = ['category', 'gender', 'material', 'price_range']\n",
    "for task in tasks:\n",
    "    train_accs = [epoch_acc[task] for epoch_acc in history['train_acc']]\n",
    "    val_accs = [epoch_acc[task] for epoch_acc in history['val_acc']]\n",
    "    axes[1].plot(val_accs, label=task.replace('_', ' ').title(), marker='o')\n",
    "\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Validation Accuracy by Task')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(MODEL_DIR) / 'training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Training curves saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Predictions\n",
    "\n",
    "Visualize predictions on sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reverse mappings\n",
    "category_names = {v: k for k, v in category_labels.items()}\n",
    "gender_names = {v: k for k, v in gender_labels.items()}\n",
    "material_names = {v: k for k, v in material_labels.items()}\n",
    "price_names = {v: k for k, v in price_labels.items()}\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(Path(MODEL_DIR) / 'best_multilabel_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Get some samples\n",
    "sample_indices = np.random.choice(len(val_dataset), 8, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, sample_idx in enumerate(sample_indices):\n",
    "    image, labels = val_dataset[sample_idx]\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        image_batch = image.unsqueeze(0).to(DEVICE)\n",
    "        outputs = model(image_batch)\n",
    "        \n",
    "        predictions = {\n",
    "            'category': category_names[outputs['category'].argmax(1).item()],\n",
    "            'gender': gender_names[outputs['gender'].argmax(1).item()],\n",
    "            'material': material_names[outputs['material'].argmax(1).item()],\n",
    "            'price_range': price_names[outputs['price_range'].argmax(1).item()]\n",
    "        }\n",
    "        \n",
    "        true_labels = {\n",
    "            'category': category_names[labels['category']],\n",
    "            'gender': gender_names[labels['gender']],\n",
    "            'material': material_names[labels['material']],\n",
    "            'price_range': price_names[labels['price_range']]\n",
    "        }\n",
    "    \n",
    "    # Display\n",
    "    img_path = val_df.iloc[sample_idx]['filepath']\n",
    "    img = Image.open(img_path)\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].axis('off')\n",
    "    \n",
    "    title = f\"True: {true_labels['category']}\\n\"\n",
    "    title += f\"Pred: {predictions['category']}\\n\"\n",
    "    title += f\"Mat: {predictions['material']} | {predictions['gender']}\"\n",
    "    \n",
    "    color = 'green' if predictions['category'] == true_labels['category'] else 'red'\n",
    "    axes[idx].set_title(title, fontsize=9, color=color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(MODEL_DIR) / 'predictions_sample.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Sample predictions visualized\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
