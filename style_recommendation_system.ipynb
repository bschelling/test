{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1243e76e",
   "metadata": {},
   "source": [
    "# üé® Style-Based Recommendation System\n",
    "## Training & Inference Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "### üìã Project Overview\n",
    "\n",
    "This notebook implements an end-to-end **customer style-based recommendation system** that:\n",
    "\n",
    "1. **Extracts visual features** from product images using MobileNetV2\n",
    "2. **Builds customer style profiles** from interaction history (weighted embeddings)\n",
    "3. **Generates personalized recommendations** based on visual similarity\n",
    "4. **Evaluates performance** using offline metrics\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Key Concepts\n",
    "\n",
    "**Customer Style Profile**: A weighted average of image embeddings from products the customer has interacted with. Products they purchased get higher weight than products they only viewed.\n",
    "\n",
    "**Recommendation Strategy**: Find products whose image embeddings are most similar (cosine similarity) to the customer's style profile.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Expected Outputs\n",
    "\n",
    "- **Product embeddings** (1280-dim vectors) for entire catalog\n",
    "- **Customer style profiles** (1280-dim vectors) for active customers\n",
    "- **Top-N recommendations** for each customer\n",
    "- **Performance metrics**: Precision@K, Recall@K, Coverage\n",
    "\n",
    "---\n",
    "\n",
    "### üóÇÔ∏è Dataset Requirements\n",
    "\n",
    "- `sample_customers.csv` - Customer profiles\n",
    "- `sample_interactions.csv` - Browsing/purchase history\n",
    "- `data/collection_images/` - Product images\n",
    "- Trained MobileNetV2 model (or use pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a7addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Mode Selection\n",
    "TEST_MODE = False  # Set to True for quick testing with limited data\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = \"/project/data\"\n",
    "IMAGE_DIR = \"/project/data/collection_images\"\n",
    "MODEL_DIR = \"/project/models\"\n",
    "OUTPUT_DIR = \"/project/outputs\"\n",
    "\n",
    "# Model Settings\n",
    "IMG_SIZE = 224\n",
    "EMBEDDING_DIM = 1280  # MobileNetV2 output dimension\n",
    "\n",
    "# Recommendation Settings\n",
    "MIN_INTERACTIONS = 3  # Minimum interactions to build customer profile\n",
    "RECENCY_HALF_LIFE_DAYS = 30  # Weight decay for older interactions\n",
    "TOP_N_RECOMMENDATIONS = 10  # Number of recommendations to generate\n",
    "\n",
    "# Interaction Weights (how much each interaction type contributes to style profile)\n",
    "INTERACTION_WEIGHTS = {\n",
    "    'purchase': 10.0,         # Strongest signal\n",
    "    'add_to_wishlist': 5.0,   # Strong intent\n",
    "    'add_to_cart': 3.0,       # Medium intent\n",
    "    'view': 1.0,              # Weak signal\n",
    "    'click': 0.5              # Weakest signal\n",
    "}\n",
    "\n",
    "# Test Mode Settings\n",
    "if TEST_MODE:\n",
    "    MAX_PRODUCTS = 50  # Limit products for testing\n",
    "    MAX_CUSTOMERS = 10  # Limit customers for testing\n",
    "    print(\"üß™ TEST MODE: Using limited data for quick validation\")\n",
    "else:\n",
    "    MAX_PRODUCTS = None  # Use all products\n",
    "    MAX_CUSTOMERS = None  # Use all customers\n",
    "    print(\"üéØ PRODUCTION MODE: Using full dataset\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"  Image Size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"  Embedding Dimension: {EMBEDDING_DIM}\")\n",
    "print(f\"  Min Interactions: {MIN_INTERACTIONS}\")\n",
    "print(f\"  Recency Half-Life: {RECENCY_HALF_LIFE_DAYS} days\")\n",
    "print(f\"  Top-N Recommendations: {TOP_N_RECOMMENDATIONS}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bb9160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORTS\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "\n",
    "# Similarity & Metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Custom modules\n",
    "sys.path.append('/project/code')\n",
    "from customer_style_profiler import CustomerStyleProfiler\n",
    "\n",
    "# Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úì Using device: {device}\")\n",
    "\n",
    "# Create output directory\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"‚úì Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789a662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD DATA\n",
    "# ============================================================\n",
    "\n",
    "print(\"üìä Loading datasets...\")\n",
    "\n",
    "# Load customer and interaction data\n",
    "customers = pd.read_csv(f\"{DATA_DIR}/sample_customers.csv\")\n",
    "interactions = pd.read_csv(f\"{DATA_DIR}/sample_interactions.csv\")\n",
    "order_items = pd.read_csv(f\"{DATA_DIR}/sample_order_items.csv\")\n",
    "\n",
    "# Convert timestamps\n",
    "interactions['timestamp'] = pd.to_datetime(interactions['timestamp'])\n",
    "\n",
    "# Get unique products that have interactions\n",
    "unique_products = interactions['product_id'].unique()\n",
    "\n",
    "# Apply test mode limits if needed\n",
    "if TEST_MODE and MAX_PRODUCTS:\n",
    "    unique_products = unique_products[:MAX_PRODUCTS]\n",
    "    interactions = interactions[interactions['product_id'].isin(unique_products)]\n",
    "\n",
    "if TEST_MODE and MAX_CUSTOMERS:\n",
    "    test_customers = customers['customer_id'].head(MAX_CUSTOMERS).values\n",
    "    interactions = interactions[interactions['customer_id'].isin(test_customers)]\n",
    "    customers = customers[customers['customer_id'].isin(test_customers)]\n",
    "\n",
    "print(f\"\\n‚úì Loaded data:\")\n",
    "print(f\"  Customers: {len(customers):,}\")\n",
    "print(f\"  Interactions: {len(interactions):,}\")\n",
    "print(f\"  Unique Products: {len(unique_products):,}\")\n",
    "print(f\"  Interaction Types: {interactions['interaction_type'].value_counts().to_dict()}\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\nüìù Sample interactions:\")\n",
    "interactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936e1652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMAGE EMBEDDING MODEL\n",
    "# ============================================================\n",
    "\n",
    "class ImageEmbeddingExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    Extract 1280-dimensional embeddings from product images using MobileNetV2.\n",
    "    This is the same architecture used in the collection prediction model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        # Load pretrained MobileNetV2\n",
    "        mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "        \n",
    "        # Use feature extraction layers (before classifier)\n",
    "        self.features = mobilenet.features\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten to (batch, 1280)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "print(\"üé® Initializing embedding model...\")\n",
    "embedding_model = ImageEmbeddingExtractor(pretrained=True).to(device)\n",
    "embedding_model.eval()\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in embedding_model.parameters())\n",
    "print(f\"‚úì Model initialized\")\n",
    "print(f\"  Architecture: MobileNetV2\")\n",
    "print(f\"  Output dimension: {EMBEDDING_DIM}\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d1ab4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXTRACT PRODUCT EMBEDDINGS\n",
    "# ============================================================\n",
    "\n",
    "# Image preprocessing pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def extract_embedding(image_path):\n",
    "    \"\"\"Extract 1280-dim embedding from a single image.\"\"\"\n",
    "    try:\n",
    "        # Load and preprocess image\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Extract embedding\n",
    "        with torch.no_grad():\n",
    "            embedding = embedding_model(img_tensor)\n",
    "        \n",
    "        return embedding.cpu().numpy().flatten()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error processing {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract embeddings for all products\n",
    "print(f\"\\nüé® Extracting embeddings for {len(unique_products)} products...\")\n",
    "print(f\"   Image directory: {IMAGE_DIR}\")\n",
    "\n",
    "product_embeddings = {}\n",
    "image_dir = Path(IMAGE_DIR)\n",
    "successful = 0\n",
    "failed = 0\n",
    "\n",
    "for product_id in tqdm(unique_products, desc='Extracting embeddings'):\n",
    "    image_path = image_dir / f\"{product_id}.jpg\"\n",
    "    \n",
    "    if image_path.exists():\n",
    "        embedding = extract_embedding(str(image_path))\n",
    "        if embedding is not None:\n",
    "            product_embeddings[str(product_id)] = embedding\n",
    "            successful += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "    else:\n",
    "        failed += 1\n",
    "\n",
    "print(f\"\\n‚úì Embedding extraction complete:\")\n",
    "print(f\"  Successful: {successful:,} products\")\n",
    "print(f\"  Failed/Missing: {failed:,} products\")\n",
    "print(f\"  Success rate: {(successful/(successful+failed)*100):.1f}%\")\n",
    "print(f\"  Embedding shape: {list(product_embeddings.values())[0].shape if product_embeddings else 'N/A'}\")\n",
    "\n",
    "# Save embeddings to disk for future use\n",
    "embeddings_file = Path(MODEL_DIR) / \"product_embeddings.npz\"\n",
    "np.savez_compressed(\n",
    "    embeddings_file,\n",
    "    product_ids=list(product_embeddings.keys()),\n",
    "    embeddings=np.array(list(product_embeddings.values()))\n",
    ")\n",
    "print(f\"\\nüíæ Saved embeddings to: {embeddings_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553efc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BUILD CUSTOMER STYLE PROFILES\n",
    "# ============================================================\n",
    "\n",
    "print(\"üë§ Building customer style profiles...\")\n",
    "\n",
    "# Initialize the profiler\n",
    "profiler = CustomerStyleProfiler(\n",
    "    product_embeddings=product_embeddings,\n",
    "    recency_half_life_days=RECENCY_HALF_LIFE_DAYS\n",
    ")\n",
    "\n",
    "# Update interaction weights\n",
    "profiler.INTERACTION_WEIGHTS = INTERACTION_WEIGHTS\n",
    "\n",
    "# Build profiles for all customers with sufficient interactions\n",
    "customer_profiles = profiler.build_all_profiles(\n",
    "    interactions_df=interactions,\n",
    "    min_interactions=MIN_INTERACTIONS\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Customer profile building complete:\")\n",
    "print(f\"  Total customers in dataset: {len(customers)}\")\n",
    "print(f\"  Customers with profiles: {len(customer_profiles)} ({len(customer_profiles)/len(customers)*100:.1f}%)\")\n",
    "print(f\"  Customers without profiles: {len(customers) - len(customer_profiles)} (insufficient interactions)\")\n",
    "print(f\"  Profile dimension: {list(customer_profiles.values())[0].shape[0] if customer_profiles else 'N/A'}\")\n",
    "\n",
    "# Analyze interaction coverage\n",
    "customers_with_profiles = list(customer_profiles.keys())\n",
    "profile_interactions = interactions[interactions['customer_id'].isin(customers_with_profiles)]\n",
    "\n",
    "print(f\"\\nüìä Profile Statistics:\")\n",
    "print(f\"  Interactions used: {len(profile_interactions):,} / {len(interactions):,} ({len(profile_interactions)/len(interactions)*100:.1f}%)\")\n",
    "print(f\"  Avg interactions per profiled customer: {len(profile_interactions)/len(customer_profiles):.1f}\")\n",
    "\n",
    "# Interaction breakdown for profiled customers\n",
    "interaction_breakdown = profile_interactions['interaction_type'].value_counts()\n",
    "print(f\"\\n  Interaction type breakdown (profiled customers):\")\n",
    "for itype, count in interaction_breakdown.items():\n",
    "    weight = INTERACTION_WEIGHTS.get(itype, 1.0)\n",
    "    print(f\"    {itype}: {count:,} (weight: {weight}x)\")\n",
    "\n",
    "# Save profiles to disk\n",
    "profiles_file = Path(MODEL_DIR) / \"customer_style_profiles.npz\"\n",
    "np.savez_compressed(\n",
    "    profiles_file,\n",
    "    customer_ids=list(customer_profiles.keys()),\n",
    "    profiles=np.array(list(customer_profiles.values()))\n",
    ")\n",
    "print(f\"\\nüíæ Saved profiles to: {profiles_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb52461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GENERATE RECOMMENDATIONS FOR ALL CUSTOMERS\n",
    "# ============================================================\n",
    "\n",
    "print(\"üéØ Generating recommendations for all customers...\")\n",
    "\n",
    "all_recommendations = {}\n",
    "\n",
    "for customer_id in tqdm(customers_with_profiles, desc=\"Generating recommendations\"):\n",
    "    # Get customer's interaction history to exclude already-purchased items\n",
    "    customer_interactions = interactions[interactions['customer_id'] == customer_id]\n",
    "    purchased_items = set(customer_interactions[customer_interactions['interaction_type'] == 'purchase']['product_id'].unique())\n",
    "    \n",
    "    # Generate recommendations\n",
    "    recommendations = profiler.recommend_by_style(\n",
    "        customer_id=customer_id,\n",
    "        customer_profiles=customer_profiles,\n",
    "        top_k=TOP_K,\n",
    "        exclude_products=purchased_items\n",
    "    )\n",
    "    \n",
    "    all_recommendations[customer_id] = recommendations\n",
    "\n",
    "print(f\"\\n‚úì Recommendation generation complete:\")\n",
    "print(f\"  Total customers: {len(all_recommendations):,}\")\n",
    "print(f\"  Recommendations per customer: {TOP_K}\")\n",
    "print(f\"  Total recommendations: {len(all_recommendations) * TOP_K:,}\")\n",
    "\n",
    "# Analyze recommendation diversity\n",
    "recommended_products = [prod_id for recs in all_recommendations.values() for prod_id, _ in recs]\n",
    "unique_recommended = len(set(recommended_products))\n",
    "total_products = len(product_embeddings)\n",
    "\n",
    "print(f\"\\nüìä Recommendation Coverage:\")\n",
    "print(f\"  Unique products recommended: {unique_recommended:,} / {total_products:,} ({unique_recommended/total_products*100:.1f}%)\")\n",
    "print(f\"  Avg recommendations per unique product: {len(recommended_products)/unique_recommended:.1f}x\")\n",
    "\n",
    "# Most frequently recommended products\n",
    "from collections import Counter\n",
    "top_recommended = Counter(recommended_products).most_common(10)\n",
    "print(f\"\\nüî• Top 10 most frequently recommended products:\")\n",
    "for rank, (prod_id, count) in enumerate(top_recommended, 1):\n",
    "    prod_info = products[products['artikel_id'] == prod_id].iloc[0] if prod_id in products['artikel_id'].values else None\n",
    "    if prod_info is not None:\n",
    "        name = prod_info['produkt_de'][:40] if 'produkt_de' in prod_info else 'Unknown'\n",
    "        print(f\"  {rank:2d}. {prod_id} - {name} ({count} customers)\")\n",
    "    else:\n",
    "        print(f\"  {rank:2d}. {prod_id} ({count} customers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d28966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUATE RECOMMENDATION PERFORMANCE\n",
    "# ============================================================\n",
    "\n",
    "print(\"üìä Evaluating recommendation performance...\")\n",
    "\n",
    "# For evaluation, we'll use a simple holdout strategy:\n",
    "# - Use older interactions to build profiles\n",
    "# - Test if recent purchases are in the top-K recommendations\n",
    "\n",
    "# Split interactions by date\n",
    "interactions_sorted = interactions.sort_values('timestamp')\n",
    "split_date = pd.to_datetime('2024-10-15')  # Use ~mid-point as split\n",
    "\n",
    "train_interactions = interactions_sorted[interactions_sorted['timestamp'] < split_date]\n",
    "test_interactions = interactions_sorted[interactions_sorted['timestamp'] >= split_date]\n",
    "test_purchases = test_interactions[test_interactions['interaction_type'] == 'purchase']\n",
    "\n",
    "print(f\"\\nüìÖ Data Split:\")\n",
    "print(f\"  Split date: {split_date.strftime('%Y-%m-%d')}\")\n",
    "print(f\"  Train interactions: {len(train_interactions):,} ({len(train_interactions)/len(interactions)*100:.1f}%)\")\n",
    "print(f\"  Test interactions: {len(test_interactions):,} ({len(test_interactions)/len(interactions)*100:.1f}%)\")\n",
    "print(f\"  Test purchases: {len(test_purchases):,}\")\n",
    "\n",
    "# Build profiles using only training data\n",
    "train_profiles = profiler.build_all_profiles(\n",
    "    interactions_df=train_interactions,\n",
    "    min_interactions=MIN_INTERACTIONS\n",
    ")\n",
    "\n",
    "print(f\"\\nüë§ Training profiles: {len(train_profiles)}\")\n",
    "\n",
    "# Calculate metrics for customers with both training profiles and test purchases\n",
    "evaluable_customers = set(train_profiles.keys()) & set(test_purchases['customer_id'].unique())\n",
    "print(f\"  Evaluable customers: {len(evaluable_customers)} (have profile + test purchases)\")\n",
    "\n",
    "if len(evaluable_customers) > 0:\n",
    "    # Calculate Precision@K and Recall@K\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    hits = []\n",
    "    \n",
    "    for customer_id in evaluable_customers:\n",
    "        # Get test purchases for this customer\n",
    "        customer_test_purchases = test_purchases[test_purchases['customer_id'] == customer_id]['product_id'].unique()\n",
    "        \n",
    "        # Get training purchases to exclude\n",
    "        customer_train_purchases = train_interactions[\n",
    "            (train_interactions['customer_id'] == customer_id) & \n",
    "            (train_interactions['interaction_type'] == 'purchase')\n",
    "        ]['product_id'].unique()\n",
    "        \n",
    "        # Generate recommendations\n",
    "        recs = profiler.recommend_by_style(\n",
    "            customer_id=customer_id,\n",
    "            customer_profiles=train_profiles,\n",
    "            top_k=TOP_K,\n",
    "            exclude_products=set(customer_train_purchases)\n",
    "        )\n",
    "        \n",
    "        recommended_ids = [prod_id for prod_id, _ in recs]\n",
    "        \n",
    "        # Calculate precision and recall\n",
    "        hits_count = len(set(recommended_ids) & set(customer_test_purchases))\n",
    "        precision = hits_count / TOP_K if TOP_K > 0 else 0\n",
    "        recall = hits_count / len(customer_test_purchases) if len(customer_test_purchases) > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        hits.append(hits_count)\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_hits = np.mean(hits)\n",
    "    hit_rate = sum(1 for h in hits if h > 0) / len(hits)\n",
    "    \n",
    "    print(f\"\\nüéØ Performance Metrics (on test set):\")\n",
    "    print(f\"  Precision@{TOP_K}: {avg_precision*100:.2f}%\")\n",
    "    print(f\"  Recall@{TOP_K}: {avg_recall*100:.2f}%\")\n",
    "    print(f\"  Hit Rate@{TOP_K}: {hit_rate*100:.2f}% ({sum(1 for h in hits if h > 0)}/{len(hits)} customers)\")\n",
    "    print(f\"  Avg hits per customer: {avg_hits:.2f}\")\n",
    "    \n",
    "    # Show distribution of hits\n",
    "    hit_distribution = Counter(hits)\n",
    "    print(f\"\\n  Hit distribution:\")\n",
    "    for hit_count in sorted(hit_distribution.keys()):\n",
    "        count = hit_distribution[hit_count]\n",
    "        print(f\"    {hit_count} hits: {count} customers ({count/len(hits)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Not enough data for meaningful evaluation\")\n",
    "    print(\"  (Need customers with both training interactions and test purchases)\")\n",
    "    \n",
    "# Recommendation diversity metrics (using full profiles)\n",
    "print(f\"\\nüåà Diversity Metrics (full dataset):\")\n",
    "print(f\"  Catalog coverage: {unique_recommended}/{total_products} products ({unique_recommended/total_products*100:.1f}%)\")\n",
    "print(f\"  Gini coefficient: {1 - (2 * sum((i+1) * count for i, (_, count) in enumerate(sorted(Counter(recommended_products).items(), key=lambda x: x[1])))) / (len(recommended_products) * len(set(recommended_products))):.3f}\")\n",
    "print(f\"    (0 = perfect equality, 1 = perfect inequality)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bcd84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INFERENCE EXAMPLES - SHOW RECOMMENDATIONS FOR SAMPLE CUSTOMERS\n",
    "# ============================================================\n",
    "\n",
    "print(\"üîç Showing detailed recommendations for sample customers...\\n\")\n",
    "\n",
    "# Select 3 customers with different profiles\n",
    "sample_customers = list(customers_with_profiles)[:3]\n",
    "\n",
    "for idx, customer_id in enumerate(sample_customers, 1):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"CUSTOMER #{idx}: {customer_id}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get customer info\n",
    "    customer_info = customers[customers['customer_id'] == customer_id].iloc[0]\n",
    "    print(f\"\\nüë§ Customer Profile:\")\n",
    "    print(f\"  Age: {customer_info['age']}\")\n",
    "    print(f\"  Gender: {customer_info['gender']}\")\n",
    "    print(f\"  Location: {customer_info['location']}\")\n",
    "    print(f\"  Segment: {customer_info['customer_segment']}\")\n",
    "    print(f\"  Lifetime Value: ‚Ç¨{customer_info['lifetime_value']:,.2f}\")\n",
    "    print(f\"  Total Orders: {customer_info['total_orders']}\")\n",
    "    \n",
    "    # Get interaction history\n",
    "    customer_interactions = interactions[interactions['customer_id'] == customer_id]\n",
    "    print(f\"\\nüìä Interaction History:\")\n",
    "    print(f\"  Total interactions: {len(customer_interactions)}\")\n",
    "    for itype, count in customer_interactions['interaction_type'].value_counts().items():\n",
    "        print(f\"    {itype}: {count}\")\n",
    "    \n",
    "    # Show recent purchases\n",
    "    recent_purchases = customer_interactions[\n",
    "        customer_interactions['interaction_type'] == 'purchase'\n",
    "    ].sort_values('timestamp', ascending=False).head(3)\n",
    "    \n",
    "    if len(recent_purchases) > 0:\n",
    "        print(f\"\\nüõçÔ∏è  Recent Purchases:\")\n",
    "        for _, purchase in recent_purchases.iterrows():\n",
    "            prod_id = purchase['product_id']\n",
    "            timestamp = purchase['timestamp']\n",
    "            prod_info = products[products['artikel_id'] == prod_id].iloc[0] if prod_id in products['artikel_id'].values else None\n",
    "            if prod_info is not None:\n",
    "                name = prod_info['produkt_de'][:50] if 'produkt_de' in prod_info else 'Unknown'\n",
    "                collection = prod_info['kollektion_de'] if 'kollektion_de' in prod_info else 'Unknown'\n",
    "                print(f\"    ‚Ä¢ {prod_id} - {name}\")\n",
    "                print(f\"      Collection: {collection} | Date: {timestamp.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    # Get recommendations\n",
    "    purchased_items = set(customer_interactions[customer_interactions['interaction_type'] == 'purchase']['product_id'].unique())\n",
    "    recommendations = all_recommendations.get(customer_id, [])\n",
    "    \n",
    "    print(f\"\\nüéØ Top {len(recommendations)} Recommendations:\")\n",
    "    for rank, (prod_id, similarity) in enumerate(recommendations, 1):\n",
    "        prod_info = products[products['artikel_id'] == prod_id].iloc[0] if prod_id in products['artikel_id'].values else None\n",
    "        if prod_info is not None:\n",
    "            name = prod_info['produkt_de'][:50] if 'produkt_de' in prod_info else 'Unknown'\n",
    "            collection = prod_info['kollektion_de'] if 'kollektion_de' in prod_info else 'Unknown'\n",
    "            price = prod_info['preis'] if 'preis' in prod_info else 'N/A'\n",
    "            print(f\"  {rank:2d}. [{similarity:.4f}] {prod_id}\")\n",
    "            print(f\"      {name}\")\n",
    "            print(f\"      Collection: {collection} | Price: ‚Ç¨{price}\")\n",
    "        else:\n",
    "            print(f\"  {rank:2d}. [{similarity:.4f}] {prod_id}\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚úì Inference examples complete\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3e3aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZE RECOMMENDATION QUALITY\n",
    "# ============================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Recommendation System Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Profile Coverage\n",
    "ax1 = axes[0, 0]\n",
    "profile_coverage = pd.DataFrame({\n",
    "    'Category': ['With Profile', 'Without Profile'],\n",
    "    'Count': [len(customer_profiles), len(customers) - len(customer_profiles)]\n",
    "})\n",
    "sns.barplot(data=profile_coverage, x='Category', y='Count', ax=ax1, palette='viridis')\n",
    "ax1.set_title('Customer Profile Coverage', fontweight='bold')\n",
    "ax1.set_ylabel('Number of Customers')\n",
    "for i, v in enumerate(profile_coverage['Count']):\n",
    "    ax1.text(i, v + 0.5, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# 2. Recommendation Diversity\n",
    "ax2 = axes[0, 1]\n",
    "product_freq = Counter(recommended_products)\n",
    "freq_distribution = Counter(product_freq.values())\n",
    "sorted_freqs = sorted(freq_distribution.items())\n",
    "ax2.bar([f for f, _ in sorted_freqs], [c for _, c in sorted_freqs], color='coral')\n",
    "ax2.set_title('Product Recommendation Frequency', fontweight='bold')\n",
    "ax2.set_xlabel('Times Recommended')\n",
    "ax2.set_ylabel('Number of Products')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Interaction Type Distribution\n",
    "ax3 = axes[1, 0]\n",
    "interaction_counts = profile_interactions['interaction_type'].value_counts()\n",
    "colors = plt.cm.Set3(range(len(interaction_counts)))\n",
    "wedges, texts, autotexts = ax3.pie(\n",
    "    interaction_counts.values,\n",
    "    labels=interaction_counts.index,\n",
    "    autopct='%1.1f%%',\n",
    "    colors=colors,\n",
    "    startangle=90\n",
    ")\n",
    "ax3.set_title('Interaction Type Distribution\\n(Profiled Customers)', fontweight='bold')\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "\n",
    "# 4. Similarity Score Distribution\n",
    "ax4 = axes[1, 1]\n",
    "all_similarities = [sim for recs in all_recommendations.values() for _, sim in recs]\n",
    "ax4.hist(all_similarities, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "ax4.axvline(np.mean(all_similarities), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(all_similarities):.4f}')\n",
    "ax4.set_title('Recommendation Similarity Scores', fontweight='bold')\n",
    "ax4.set_xlabel('Cosine Similarity')\n",
    "ax4.set_ylabel('Frequency')\n",
    "ax4.legend()\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Visualization Summary:\")\n",
    "print(f\"  Profile coverage: {len(customer_profiles)}/{len(customers)} ({len(customer_profiles)/len(customers)*100:.1f}%)\")\n",
    "print(f\"  Unique products recommended: {unique_recommended}/{total_products} ({unique_recommended/total_products*100:.1f}%)\")\n",
    "print(f\"  Mean similarity score: {np.mean(all_similarities):.4f}\")\n",
    "print(f\"  Median similarity score: {np.median(all_similarities):.4f}\")\n",
    "print(f\"  Min similarity score: {np.min(all_similarities):.4f}\")\n",
    "print(f\"  Max similarity score: {np.max(all_similarities):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39659561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE MODELS AND ARTIFACTS FOR DEPLOYMENT\n",
    "# ============================================================\n",
    "\n",
    "print(\"üíæ Saving models and artifacts...\")\n",
    "\n",
    "# 1. Save product embeddings (already saved during extraction)\n",
    "print(f\"‚úì Product embeddings: {embeddings_file}\")\n",
    "\n",
    "# 2. Save customer profiles (already saved)\n",
    "print(f\"‚úì Customer profiles: {profiles_file}\")\n",
    "\n",
    "# 3. Save recommendations to JSON for easy deployment\n",
    "import json\n",
    "\n",
    "recommendations_file = Path(MODEL_DIR) / \"recommendations.json\"\n",
    "recommendations_json = {\n",
    "    customer_id: [\n",
    "        {\"product_id\": prod_id, \"similarity\": float(sim)}\n",
    "        for prod_id, sim in recs\n",
    "    ]\n",
    "    for customer_id, recs in all_recommendations.items()\n",
    "}\n",
    "\n",
    "with open(recommendations_file, 'w') as f:\n",
    "    json.dump(recommendations_json, f, indent=2)\n",
    "print(f\"‚úì Recommendations: {recommendations_file}\")\n",
    "\n",
    "# 4. Save metadata\n",
    "metadata = {\n",
    "    \"model_type\": \"MobileNetV2\",\n",
    "    \"embedding_dim\": EMBEDDING_DIM,\n",
    "    \"image_size\": IMG_SIZE,\n",
    "    \"test_mode\": TEST_MODE,\n",
    "    \"min_interactions\": MIN_INTERACTIONS,\n",
    "    \"top_k\": TOP_K,\n",
    "    \"recency_half_life_days\": RECENCY_HALF_LIFE_DAYS,\n",
    "    \"interaction_weights\": INTERACTION_WEIGHTS,\n",
    "    \"total_customers\": len(customers),\n",
    "    \"customers_with_profiles\": len(customer_profiles),\n",
    "    \"total_products\": len(products),\n",
    "    \"products_with_embeddings\": len(product_embeddings),\n",
    "    \"unique_products_recommended\": unique_recommended,\n",
    "    \"catalog_coverage_pct\": round(unique_recommended/total_products*100, 2),\n",
    "    \"timestamp\": pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "metadata_file = Path(MODEL_DIR) / \"recommendation_metadata.json\"\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"‚úì Metadata: {metadata_file}\")\n",
    "\n",
    "# 5. Save profiler configuration for easy reload\n",
    "profiler_config = {\n",
    "    \"recency_half_life_days\": RECENCY_HALF_LIFE_DAYS,\n",
    "    \"interaction_weights\": INTERACTION_WEIGHTS,\n",
    "    \"min_interactions\": MIN_INTERACTIONS\n",
    "}\n",
    "\n",
    "profiler_config_file = Path(MODEL_DIR) / \"profiler_config.json\"\n",
    "with open(profiler_config_file, 'w') as f:\n",
    "    json.dump(profiler_config, f, indent=2)\n",
    "print(f\"‚úì Profiler config: {profiler_config_file}\")\n",
    "\n",
    "print(f\"\\nüì¶ All artifacts saved to: {MODEL_DIR}\")\n",
    "print(f\"\\nTo load and use the recommendation system:\")\n",
    "print(f\"\"\"\n",
    "# Load embeddings\n",
    "data = np.load('{embeddings_file}')\n",
    "product_embeddings = dict(zip(data['product_ids'], data['embeddings']))\n",
    "\n",
    "# Load profiles\n",
    "profile_data = np.load('{profiles_file}')\n",
    "customer_profiles = dict(zip(profile_data['customer_ids'], profile_data['profiles']))\n",
    "\n",
    "# Load config\n",
    "with open('{profiler_config_file}', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Initialize profiler\n",
    "profiler = CustomerStyleProfiler(\n",
    "    product_embeddings=product_embeddings,\n",
    "    recency_half_life_days=config['recency_half_life_days']\n",
    ")\n",
    "profiler.INTERACTION_WEIGHTS = config['interaction_weights']\n",
    "\n",
    "# Generate recommendations\n",
    "recommendations = profiler.recommend_by_style(\n",
    "    customer_id='C001',\n",
    "    customer_profiles=customer_profiles,\n",
    "    top_k=10\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227dede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SUMMARY & NEXT STEPS\n",
    "# ============================================================\n",
    "\n",
    "print(\"üéâ Style-Based Recommendation System - Complete!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä FINAL RESULTS:\")\n",
    "print(f\"  ‚Ä¢ Customers analyzed: {len(customers):,}\")\n",
    "print(f\"  ‚Ä¢ Customers with profiles: {len(customer_profiles):,} ({len(customer_profiles)/len(customers)*100:.1f}%)\")\n",
    "print(f\"  ‚Ä¢ Products in catalog: {len(products):,}\")\n",
    "print(f\"  ‚Ä¢ Products with embeddings: {len(product_embeddings):,}\")\n",
    "print(f\"  ‚Ä¢ Unique products recommended: {unique_recommended:,} ({unique_recommended/total_products*100:.1f}% coverage)\")\n",
    "print(f\"  ‚Ä¢ Recommendations per customer: {TOP_K}\")\n",
    "print(f\"  ‚Ä¢ Mean similarity score: {np.mean(all_similarities):.4f}\")\n",
    "\n",
    "if len(evaluable_customers) > 0:\n",
    "    print(f\"\\nüéØ EVALUATION METRICS (Test Set):\")\n",
    "    print(f\"  ‚Ä¢ Precision@{TOP_K}: {avg_precision*100:.2f}%\")\n",
    "    print(f\"  ‚Ä¢ Recall@{TOP_K}: {avg_recall*100:.2f}%\")\n",
    "    print(f\"  ‚Ä¢ Hit Rate@{TOP_K}: {hit_rate*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nüíæ SAVED ARTIFACTS:\")\n",
    "print(f\"  ‚Ä¢ {embeddings_file}\")\n",
    "print(f\"  ‚Ä¢ {profiles_file}\")\n",
    "print(f\"  ‚Ä¢ {recommendations_file}\")\n",
    "print(f\"  ‚Ä¢ {metadata_file}\")\n",
    "print(f\"  ‚Ä¢ {profiler_config_file}\")\n",
    "\n",
    "print(\"\\nüöÄ NEXT STEPS:\")\n",
    "print(\"\"\"\n",
    "1. Production Deployment:\n",
    "   - Integrate with e-commerce platform API\n",
    "   - Set up batch processing for nightly profile updates\n",
    "   - Implement real-time recommendation serving (< 50ms latency)\n",
    "   - Add caching layer (Redis) for frequently accessed recommendations\n",
    "\n",
    "2. Model Improvements:\n",
    "   - A/B test different interaction weights\n",
    "   - Experiment with recency decay parameters\n",
    "   - Try ensemble with collaborative filtering\n",
    "   - Add demographic filters (age, gender, location)\n",
    "   - Incorporate seasonal trends (holidays, seasons)\n",
    "\n",
    "3. Monitoring & Analytics:\n",
    "   - Track click-through rate (CTR) on recommendations\n",
    "   - Measure conversion rate and revenue impact\n",
    "   - Monitor recommendation diversity over time\n",
    "   - Set up alerts for coverage drops or quality degradation\n",
    "\n",
    "4. Advanced Features:\n",
    "   - \"Shop the Look\" - recommend complete outfits\n",
    "   - \"Similar Customers\" - show what similar shoppers bought\n",
    "   - \"Trending in Your Style\" - personalized trending items\n",
    "   - Email campaigns with personalized recommendations\n",
    "   - \"Customers also bought\" - order-based recommendations\n",
    "\n",
    "5. Data Collection:\n",
    "   - Gather more customer interaction data\n",
    "   - Collect explicit feedback (ratings, likes)\n",
    "   - Track recommendation performance metrics\n",
    "   - Build feedback loop for continuous improvement\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚úì Notebook execution complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e214eb7",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Summary & Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3da429",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Save Models & Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b0b59d",
   "metadata": {},
   "source": [
    "## üîü Visualize Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee01a3ab",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Inference Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df42ec75",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Evaluate Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f5999c",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Generate Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd32ca85",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Build Customer Style Profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1305d1d",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Extract Product Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0369dc0",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Build Image Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12de50",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb8259f",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51114281",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Configuration"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
